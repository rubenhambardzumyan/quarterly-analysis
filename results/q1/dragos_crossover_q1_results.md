# Dragos Q1 Plan Analysis - BrainLift Opportunities

*Context: No BrainLifts were created in Q4.*

---

## Tab 1: Hiring Services

### Goal 1: Time-to-fill consistency while supporting Alpha expansion (10x growth in 2026)

#### Tactic: "Scale the hiring team" (Hire two VP OrgBuilders)

**No BrainLift Suggested - Procurement Decision**

This is a hiring decision, not an expertise gap.

---

#### Tactic: "Continue moving work to AI" (AIPIs, automations, eliminate admin work)

**Proposed BrainLift: AI-Enabling Hiring Operations**

**Purpose Statement:**
The hiring team has successfully moved significant work to AI (77% AI grading, AIPIs, simulation-based assessments, video grading) but this expertise exists as tacit knowledge rather than documented methodology. The Q1 plan calls for continuing to "code practices into AIPIs," "expose AIPIs to customers," and "eliminate admin work" - all requiring judgment about what to automate, how to design AI-graded assessments that maintain quality, and how to validate AI decisions. The expertise gap is in having a systematic framework: criteria for identifying automation candidates, patterns for effective AI assessment design, and quality validation approaches. This BrainLift would formalize the team's accumulated expertise into reusable methodology.

**Scope:**
- In scope: Criteria for identifying AI-automation candidates in hiring, assessment design patterns for AI grading, quality validation approaches, lessons from implementations (video grading, proctoring, simulations)
- Out of scope: Specific technical implementations, vendor selection, cost optimization

---

#### Tactic: "Improve completion rates (primarily BFQ)"

**No BrainLift Suggested - Execution Task**

The problem is identified ("perceived simplicity"), the solution is known (UX/messaging changes). This is optimization work.

---

#### Tactic: "Scale hypertargeted recruitment" / "Explore Local/In-Person Recruiting Tactics"

**No BrainLift Suggested - Experimentation, Not Expertise Gap**

The document describes testing three approaches (passive touchpoints, local contractors, dedicated traveler) to learn what works. This is structured experimentation - appropriate when you don't know which approach fits best, but doesn't indicate a fundamental expertise gap. The team isn't saying "we don't know how to recruit locally" - they're saying "we need to test which model works for our context."

---

#### Tactic: "Leverage and reflect Alpha's brand as great employee branding for education roles"

**No BrainLift Suggested - Execution/Marketing Task**

The document describes content creation activities (testimonials, stories, articles) and channel improvements. The team knows education candidates who see content find it valuable - the gap is production and distribution, not expertise. No language suggesting they don't know *what* resonates with education talent.

---

#### Tactic: "Increase AI-grading coverage" / "Plagiarism detection" / "Improve domain-specific AI-based assessments"

**No BrainLift Suggested - Execution Task**

Extensions of demonstrated capabilities. Adding video grading and plagiarism detection are feature implementations.

---

### Goal 2: Maintain 90-days retention, improve 180-days retention

#### Tactic: "Filter for culture fit"

**Question:** The document states "many edu terminations call out culture fit, misaligned values" but doesn't describe what "culture fit" means for education roles or what assessment approaches have been tried and failed. What specific expertise is missing - defining culture fit for education, or assessing it during hiring?

---

#### Tactic: "Filter for non-easily blockable"

**Proposed BrainLift: Assessing Autonomy and Ownership in Remote Hiring**

**Purpose Statement:**
Roughly 50% of terminations across all BUs cite new hires "not following through, needing constant hand-holding, not escalating blockers, leaving work not done, not completing work independently." The document identifies this as requiring assessment of "a mix of autonomy, focus and resourcefulness" - traits current hiring filters fail to predict. The team has identified the behaviors causing terminations but lacks methods to assess these traits pre-hire. This BrainLift would develop assessment frameworks and behavioral indicators that predict autonomy and ownership in remote work.

**Scope:**
- In scope: Behavioral indicators of autonomy/ownership/resourcefulness, assessment design, interview frameworks, simulation approaches for these traits
- Out of scope: Onboarding design, manager coaching, post-hire performance management

---

#### Tactic: "Work with HMs to improve onboarding"

**Proposed BrainLift: Remote Team Onboarding Design**

**Purpose Statement:**
Many terminations cite issues "easily preventable by setting proper expectations and providing team-specific guidelines" during onboarding. The document explicitly states goals to "establish a quality bar for training/onboarding material" and create "ramp-up training documentation" - clear knowledge formalization language. The team needs to define what good onboarding looks like and how to capture team-specific implicit knowledge. This BrainLift would develop frameworks for onboarding quality standards and knowledge capture methods.

**Scope:**
- In scope: Quality bar definition for onboarding materials, team-specific knowledge capture approaches, expectation-setting frameworks
- Out of scope: AI bot implementation, LMS tooling, HR compliance onboarding

---

#### Tactic: "Collect rich termination feedback"

**No BrainLift Suggested - Tooling/Process Improvement**

The document describes deploying an AI-based conversation process to collect better feedback. The challenge identified (">30% of feedback cannot be used") is about feedback quality, and the solution is a better collection mechanism. This is process improvement, not an expertise gap.

---

## Tab 2: BrainLift Coach

### Q4 Learning: "Q4 turned out to be a discovery phase... we're solving an organizational alignment problem, not a product problem"

**No BrainLift Suggested - Insight Without Clear Expertise Gap**

This is a valuable strategic insight, but the document doesn't identify a specific expertise gap to address. The Q1 response is tactical (focus on aligned users, integrate into onboarding, white glove service) rather than "we need to develop expertise in X." The team is pivoting strategy based on learnings, not identifying knowledge they need to build.

---

#### Tactic: "Integrate into Crossover onboarding"

**No BrainLift Suggested - Execution Task**

Product deployment into existing workflow. The team has delivered the course before.

---

#### Tactic: "Focus on aligned users" / "Work with invested leaders/managers"

**Proposed BrainLift: Community Building and Management**

**Purpose Statement:**
BrainLift Coach adoption requires nurturing a community of engaged users - the Q1 plan explicitly aims to "grow and nurture the community - share lessons, approaches that work or do not, success stories" and work with invested leaders to "grow the habit of building brainlifts in their teams." Building and sustaining a community of practice is a distinct expertise that differs from standard product development. The team needs to understand what makes communities of practice thrive: how to identify and activate champions, what content and interactions drive engagement, how to create peer-to-peer value, and how to scale community without losing intimacy. This BrainLift would develop expertise in community building before attempting to establish one.

**Scope:**
- In scope: Community design principles, champion identification and activation, engagement mechanisms, content strategies for communities of practice, scaling approaches
- Out of scope: BrainLift Coach product features, curriculum content, individual user coaching

---

## Tab 3: WorkSmart

### Q1 Strategy: Cover operational costs, encourage adoption, limit development

**No BrainLift Suggested - Business/Operational Decision**

The strategy is about cost management and encouraging experimentation, not expertise development.

---

### Use-case: "Jozsef / Central Eng: Observability for work units"

**Proposed BrainLift: Work Unit Observability for Engineering Teams**

**Purpose Statement:**
Managing large engineering teams requires understanding how work actually happens beyond what's visible in code or tickets. The document frames this as needing "observability" analogous to software observability - "define work units, identify deviations, learn from patterns" to "focus managers on what matters most." The team wants to apply observability concepts to team management but needs to define what that means in practice. This BrainLift would develop the framework for what work units to define, what deviations matter, and how to create learning loops.

**Scope:**
- In scope: Work unit definitions for engineering, deviation patterns worth monitoring, intervention triggers, learning loop design
- Out of scope: WorkSmart AI implementation, individual performance metrics, privacy/compliance

---

### Use-case: "Central TPM / WSEng: Capture hidden decisions"

**Proposed BrainLift: Engineering Decision Capture and Institutionalization**

**Purpose Statement:**
Engineers and TPMs make important decisions beyond what's visible in code or specs - architectural choices, tradeoff evaluations, debugging approaches - and these decisions are lost to the organization. The document explicitly states "the big opportunity is uncovering hidden decisions that engineers/TPMs make... capture these decisions, share them with the relevant group, and learn from them." This BrainLift would develop methodology for identifying which decisions matter, how to capture them without disrupting flow, and how to make them discoverable.

**Scope:**
- In scope: Decision taxonomy for engineering/TPM work, capture mechanisms, categorization frameworks, retrieval patterns
- Out of scope: Tooling implementation, performance evaluation, code review processes

---

### Use-case: "Colin / Support: Onboarding and coaching new support engineers"

**Question:** The document states managers "lack time for hands-on coaching" in the first 2-3 weeks. Is the problem that the team doesn't know how to effectively onboard support engineers at scale (expertise gap), or that they know what to do but lack bandwidth (capacity gap)? What would need to be learned to make this work?

---

### Use-case: "Jamie / Ops: Acquisition integration coordination"

**No BrainLift Suggested - Visibility/Tooling Need**

The document describes wanting visibility into "what got done yesterday, what are big open items, where are people blocked." This is a reporting/dashboard need - the team knows what information they want, they just need a mechanism to surface it. Not an expertise gap.
