# Crossover Problems Decomposition

### Intro

The goal of this document is to break down the Q4/2025 plan of Crossover into problems to analyze BrainLift opportunities for each problem and present either a purpose statement or questions to clarify before a BrainLift with a purpose statement can be presented.

### Thought Process

- **The Priority**
  - Identify good goals, and use those good goals to articulate a good purpose statement in cases a BrainLift can be suggested
  - Using questions to understand the goals deeper and improve the goals, thus being able to suggest a BrainLift with a good purpose statement
- **Goal Extraction & Validation**
  - Extract all stated goals from the document
  - For each goal, assess whether it is actionable as-is or requires decomposition:
    - Can specific problems or decisions be identified from the document context?
    - Is the goal describing a problem to solve, or merely a desired outcome?
    - Does the document provide sufficient context about current state, challenges, or approach?
    - Are there enough context and understanding to decompose it into complete, actionable work sub goals?
- **Problem Identification**
  - Extract underlying problems from:
    - Explicit problem statements in the document.
    - Challenges mentioned in context.
    - Gaps between current state and goal.
  - Avoid inventing problems not present in the document.
- **Problem Analysis**
  - For each identified problem, assess clarity:
    - If clearly defined:
      - Determine whether solving it requires developing expertise (not just executing tasks).
        - If expertise development is needed, then propose a BrainLift with clear purpose statement and use the context to articulate a clear, high quality purpose. In case of gaps, list the curious questions that need to be answered and prepare to clarify them with the user
        - If execution only, then note that it is an execution task, not a BrainLift candidate.
    - If not clearly defined:
      - List specific curious questions needed to clarify:
        - What is the current state?
        - What are the actual challenges?
        - What has been tried?
        - What decisions need to be made?
        - What expertise is lacking?
- **Quality Standards**
  - Never propose a BrainLift without a clear, specific purpose statement.
  - If the purpose statement merely restates the goal, insufficient context exists.
  - When in doubt, ask clarifying questions rather than make assumptions

### **Crossover Q4 Goals**

- **Goal 1 (as stated): Improve Hiring Operations**
  - Target as specified in the document:
    - 80% fill rate at the 45 days mark for in-model roles, with LinkedIn sourcing
    - 100% of pipelines "in-model" (this goal is not full control of Crossover, but measuring it and working to push it higher is the the right thing to do)
  - Identified problems:
    - Unclear organizational structure leads to hiring inefficiencies
      - Possible BrainLift: Organization discovery & alignment framework for new BUs
      - Purpose: Unclear organizational structure leads to duplicate orders, missing orders, over/under hiring, and hiring for wrong locations. The document explicitly states that Education faced challenges with "unclear org structure, work unit plan, pay rates, mapping to industry roles & expertise, orders, DRIs." These structural gaps caused concrete operational problems in Q3. This BrainLift captures the expertise that OrgBuilders developed when defining work units and supporting Education's growth, transforming it into a standardized process that can be applied to future business units to prevent these issues.
        - In scope:
          - Discovery session methodology for extracting org structure from stakeholders
          - Framework for mapping work units to industry roles and expertise levels
          - Process for identifying Accountable Business Executives (ABEs) for each BU/CF
          - Methodology for translating business goals into clear hiring orders with defined pay rates and locations
        - Out of scope:
          - General organizational design theory
          - Business unit strategy or product roadmaps
          - Existing, well-defined org structures that just need execution
          - Post-hire onboarding or organizational change management
    - Fraud and plagiarism waste interviewer time and erode stakeholder confidence
      - Possible BrainLift: AI-powered fraud detection framework for hiring assessments
      - Purpose: The document clearly states that "fraud muddles assessment signals and erodes stakeholder confidence in the quality of candidates reaching the interview stage." While PCCAT defends against hiring fraudsters, these candidates "still waste interviewers' time and increase their skepticism of non-fraudulent candidates." The goal is to fail fraudulent candidates pre-interview to "save interviewers' time and reduce their hypervigilance levels during interviews." This requires developing detection capabilities across multiple assessment formats (CodeSignal, proprietary AI-assisted coding assessments, GDoc-based RWAs) and extending AI-led screening to detect cross-session misrepresentation. The expertise needed is building effective fraud detection that maintains stakeholder confidence while minimizing false positives that could reject legitimate candidates.
        - In scope:
          - Framework for AI-based fraud detection across different assessment types (CodeSignal, coding assessments, RWAs)
          - Methodology for cross-session fact verification to detect misrepresentation
          - Approach to maintaining interviewer confidence while reducing false fraud signals
          - Decision criteria for flagging candidates for fraud vs allowing them to proceed
        - Out of scope:
          - General AI/ML model training or technical implementation details
          - Legal compliance or fraud prosecution procedures
          - Assessment design or question creation
          - Post-interview verification or background checks
    - SME dependency creates grading backlogs that stall hiring pipelines
      - Possible BrainLift: Autonomous AI grading quality assurance framework
      - Purpose: The document states that while Crossover has "the capability to enable AI grading for customer RWAs," they've been "rolling it up in close alignment with SMEs (establishing rules, calibration, sign-off)." This approach "works well for involved hiring managers" but "can still stall the process" when SMEs are unavailable or unresponsive. The proposed solution is to "independently move to AI grading (starting from low-bar to full-grading) after a batch of 15-20 submissions, without being blocked on SME feedback." This requires developing expertise in: when AI grading quality is sufficient to proceed autonomously, how to progressively transition from low-bar to full grading, and how to establish quality standards without continuous SME validation while keeping pipelines functioning.
        - In scope:
          - Framework for assessing AI grading readiness after initial submission batches (15-20 submissions)
          - Methodology for progressive grading transitions (low-bar to full-grading)
          - Quality assurance criteria that enable autonomous operation without SME sign-off
          - Decision framework for when to escalate to SME vs proceed with AI grading
        - Out of scope:
          - AI grading algorithm development or technical implementation
          - RWA question design or assessment creation
          - SME recruitment, training, or management
          - Handling edge cases that clearly require human judgment
    - Interview feedback delays persist despite escalation emails
      - No BrainLift needed, since this is a behavioral/process issue about getting hiring managers to respond faster. The document states "delays in interview feedback remain a persistent issue, despite escalation emails." The proposed solutions are execution-focused: presenting HMs with relevant information, consolidating information across interviewers, providing GChat prep notes, and replacing Read.ai with their own bot. While these involve AI tools, the core problem is behavioral (getting people to respond to interviews more quickly) rather than developing expertise in new judgment frameworks or standards.

- **Goal 2 (as stated): Improve Education Hiring**
  - Target as specified in the document:
    - Reduce time-to-offer, cut mishires, raise screenâ†’interview pass quality, and sustain interviews per week (T4W) above plan
  - EDU's top issues: attention, mishires, vague requirements, and niche roles
  - Identified problems:
    - Vague or subjective requirements cause hiring bottlenecks
      - Possible BrainLift: Standardized education role filters and quality bars
      - Purpose: The document identifies "vague requirements" as one of EDU's top hiring issues contributing to extended time-to-offer and mishires. Specific examples from Q3 include "persona-based screening for 2HL & Core Edu roles (personal, lived experience + mission alignment)" and "hyper-specific Ops spec (MBB alum)." These represent requirements that hiring managers understand implicitly but haven't documented, forcing recruiters to guess what makes candidates suitable. Additionally, niche education roles lack standardized filters, meaning each role gets reinvented from scratch. The proposed solution includes "Brainlifts: standardize role filters and implicit bars to improve requirement clarity and speed." This requires developing expertise in extracting implicit quality standards from experienced hiring managers and translating vague requirements into clear, reusable filters.
        - In scope:
          - Framework for translating vague education role requirements into clear, actionable filters
          - Decision criteria for when requirements are "clear enough" vs need more specificity
          - Methodology for documenting implicit quality bars from experienced education hiring managers
          - Approach to handling persona-based requirements (mission alignment, lived experience) systematically
        - Out of scope:
          - General job description writing or copywriting techniques
          - Requirements standardization for non-education roles (engineering, sales, etc.)
          - Candidate sourcing strategies or channel selection
          - Interview design or assessment creation
    - Unclear how proposed activities solve defined problems
      - No BrainLift to propose, since the document lists activities (hire VP OrgBuilder at $4.2K/wk, Outbound Recruiting Engine at $19.5K-48K, Org Discovery & Alignment workshops, Interviewer Clone Pilot, Brainlifts) but doesn't clearly explain how each addresses the specific problems (attention, mishires, vague requirements, niche roles).
      - Questions to clarify:
        - On problems:
          - What's the definition of success for each problem pattern (attention, mishires, vague requirements, niche roles)?
          - How exactly will hiring a VP OrgBuilder address the mishires problem?
          - How exactly will the Outbound Recruiting Engine reduce time-to-offer or improve requirement clarity?
        - On assumptions:
          - Is the assumption that mishires are primarily caused by vague requirements, or are there other root causes?
          - What evidence exists that the current OrgBuilder capacity is the bottleneck vs other factors?
          - How will the $125-200K budget be allocated across these activities?

- **Goal 3 (as stated): Align on relying primarily on LinkedIn; Stop Searching for Alternatives**
  - Target as specified in the document:
    - Secure executive buy-in for a (multi)yearly contract with LinkedIn
  - Identified problems:
    - Expired hiring slots limit capacity to 200 for EDU only
      - No BrainLift needed, since this is about securing executive buy-in and creating a purchase request document (LinkedIn Contract Negotiation Approval Request), not about developing expertise. The goal is organizational alignment and contract negotiation.
    - Short-term Q4 strategy contradicts long-term LinkedIn-only strategy
      - No BrainLift to propose, since the document states the long-term strategy is "relying on LinkedIn as the main sourcing system, acknowledging that alternative channels are low quality, higher cost and leave important talent segments not covered," but Q4 will use Indeed and Google Ads. The strategic reasoning isn't clearly explained.
      - Questions to clarify:
        - On problem:
          - What is the actual challenge of getting executive buy-in beyond creating the approval request document?
          - What evidence or business case is needed to secure the multi-year contract?
        - On assumptions:
          - How solid is the acknowledgement that LinkedIn should be the only source of hiring?
          - What specific evidence demonstrates that "alternative channels are low quality, higher cost and leave important talent segments not covered"?
          - How does relying on Indeed and Google Ads in Q4 support (or undermine) the case for LinkedIn-only strategy?

- **Goal 4 (as stated): Scale BrainLift Coach**
  - Target as specified in the document:
    - Increase adoption inside Trilogy - 80% of all Trilogy users spending time on BrainLifts find value in the product
  - Identified problems:
    - Target is an outcome without clarity on what's missing
      - No BrainLift to propose, since the goal lacks sufficient context about what problems need solving to achieve 80% adoption. The document shows current stats (141 registered users, 383 BrainLifts created, 35 weekly active users) and mentions user feedback ("at times overwhelming or unaware of the limitations of scope"), but doesn't clearly articulate what specific problems prevent adoption or what expertise is needed.
      - Questions to clarify:
        - On target:
          - What does "spending time on BrainLifts" mean? Is it creating, reading, or using them with AI?
          - How is "find value" measured? Is it self-reported or based on behavioral data?
          - What is the current adoption rate and why is 80% the specific target?
        - On current state:
          - Are 141 registered users, 383 BrainLifts, and 35 weekly active users considered success or failure?
          - What specific problems does "at times overwhelming or unaware of the limitations of scope" cause for users?
          - What user research or data exists about why people don't use BrainLifts or don't find value?
        - On proposed improvements:
          - How will WorkSmart AI recommendations, meeting context leverage, and IDE integration address the adoption/value problem?
          - What's the hypothesis about why technical roles spending time in IDEs would increase adoption?

- **Goal 5 (as stated): Make WorkSmart Vision AI Valuable**
  - Target as specified in the document:
    - 2 core functions using it to make decisions or generate insights
  - Current Reality:
    - Most customers only use basic time tracking
    - Full behavioral tracking exists but isn't activated
    - Strategy has been making WorkSmart AI ontology available via AIPIs but "we do not have clear value proposition that makes customers say 'I need this'"
  - Identified problems:
    - Solution in search of a problem
      - No BrainLift to propose, since the document acknowledges "we do not have clear value proposition that makes customers say 'I need this'" but doesn't clearly articulate what customer problem WorkSmart Vision AI solves. The proposed use cases (Central Eng for AI use coaching, Central SaaS for product activity tracking) are mentioned but lack context about what challenges these teams currently face.
      - Questions to clarify:
        - On target:
          - Why is "2 core functions using it" the success metric? Why not 1 or 5?
          - What happens if you achieve this target? How does it benefit Crossover or the core functions?
        - On customer needs:
          - Why do most customers only use basic time tracking instead of behavioral tracking?
          - Why isn't full behavioral tracking activated - is it because it's not ready, customers don't want it, or something else?
          - What research was done with Central Eng and Central SaaS to understand their problems?
        - On use cases:
          - For "Explain the HOW behind the WHAT" - what specific decisions would this enable that can't be made today?
          - What problems do Central Eng and Central SaaS currently face that WorkSmart Vision AI would solve?
          - For hiring process support - what specific information about hires would be valuable and why?

- **Goal 6 (as stated): Make WorkSmart the People Source of Truth**
  - Target as specified in the document:
    - Support the COO and SaaS group by making XO Manage the source of truth of all contractors working with Trilogy businesses
  - Identified problems:
    - Goal doesn't articulate the problem being solved
      - No BrainLift to propose, since the document doesn't explain what problem is being solved by having a "people source of truth" or why this is worth prioritizing in Q4. It mentions "data cleaning and cross verifications with the BU" relying on COO group support, but doesn't articulate what decisions or actions this would enable.
      - Questions to clarify:
        - On problem:
          - What is the problem of not having this? What goes wrong today without a people source of truth?
          - Why is it worth investing in this in Q4 specifically?
          - What actions or decisions would the COO and SaaS group take with this information that they can't do today?
        - On implementation:
          - What data cleaning challenges are expected? Are these technical challenges or data quality issues?
          - What's the COO group's role - subject matter expertise, coordination, or data validation?
          - What does "source of truth" mean specifically? What data fields or relationships need to be accurate?

- **Goal 7 (as stated): Connect Brand investment to business value**
  - Target as specified in the document:
    - Prove brand impact on hiring metrics and optimize spend for maximum ROI
  - Investment outcomes listed:
    - Glassdoor: 4.1/5.0 (up from 1.8 in 2021)
    - Radically Remote newsletter: 2.5M subscribers (#11 newsletter on LinkedIn)
    - 50% increase in non-branded search, 41% increase in AI search traffic
    - Content Champions: 4.2x engagement improvement
  - Identified problems:
    - Goal doesn't specify what problem it solves
      - No BrainLift to propose, since the document states they will "pragmatically focus our investment on quantifiable business value" with activities like "baseline current impact, target problem segments, optimize spend," but doesn't articulate what problem needs solving or what decisions will be made differently.
      - Questions to clarify:
        - On problem:
          - What is the actual challenge - lack of budget approval? Unclear where to invest? Poor conversion despite good brand metrics?
          - The document says "pragmatically focus our investment on quantifiable business value" - what was the focus before, and why is it changing now?
          - What specific problem segments are you targeting (the document says "//tbd")?
        - On target:
          - What does success look like? What happens when brand impact is proven?
          - Is this about getting more budget, defending existing budget, or redirecting budget to different initiatives?
          - What decisions will be made differently based on proving brand impact?
        - On approach:
          - What hypothesis exists about which problem segments need brand content vs other interventions?
          - How will you measure impact on completion rates, acceptance rates, and perception?
